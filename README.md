# KNN from Scratch and Comparison with Ensemble Algorithms

This project implements the K-Nearest Neighbors (KNN) algorithm from scratch on an imbalanced employee attrition dataset.
After identifying the best number of neighbors using the accuracy score, the dataset is also evaluated using other ensemble
learning algorithms such as Random Forest, AdaBoost, and Bagging with KNN. Finally, the accuracies of all models are
compared to analyze performance differences. It is shown that boosting algorithms, such as AdaBoost, perform slightly better
than the others.




## requirements
[python](https://www.python.org/downloads/)

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install packages.

```bash
pip install numpy
pip install scikit-learn
pip install matplotlib
pip install seaborn
pip install pandas
```

## how to run
To run this Jupyter Notebook, you can either open it in Google Colab using the link below or run it locally using Jupyter Notebook.

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farinazzarei/knn/blob/main/knn-from-scratch.ipynb)

Install Jupyter Notebook (if you don't have it yet):

```bash
pip install notebook
```
clone this repository :
```bash
git clone https://github.com/farinazzarei/knn.git
cd knn
```
Launch Jupyter Notebook:
```bash
jupyter notebook
```
open the notebook file in your browser 
